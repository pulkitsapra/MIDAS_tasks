{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Import Necessary Modules Used throughout the task \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "from tweepy import Cursor\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from collections import Counter\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. To begin the process we need to register our client application with Twitter and create a new application.\n",
    "2. The below block is used to perform authentication after creating necessary tokens.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables that contains the user credentials to access Twitter API \n",
    "access_token = \"\"\n",
    "access_token_secret = \"\"\n",
    "consumer_key = \"\"\n",
    "consumer_secret = \"\"\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "auth_api = API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Create the account for which we want to fetch the Tweets.\n",
    "2. To help make pagination easier and require less code Tweepy has the Cursor object.\n",
    "3. For all statuses, i take its json response and append into a list\n",
    "4. Finlly, The array of Json object is written to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_list=[\"midasIIITD\"]\n",
    "\n",
    "d=[]\n",
    "for status in Cursor(auth_api.user_timeline, id=account_list[0]).items():\n",
    "    d.append(status._json)\n",
    "\n",
    "with open('data_json.txt', 'w') as outfile:  \n",
    "    json.dump(d,outfile)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This block loads data from the file into jsondata *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_json.txt') as f:\n",
    "    jsondata = json.load(f)\n",
    "\n",
    "#print jsondata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The objective is to create a ndarray of number of tweets X 5. An example is shown below *\n",
    "\n",
    "| Text | Date                         |Likes| Retweet | Image Count |\n",
    "|------|------------------------------|-----|---------|-------------|\n",
    "|asdfaa|Sat Apr 06 17:11:29 +0000 2019| 0   |   5     |    None     |\n",
    "\n",
    "1. Create an empty list to store each tweet's filtered results\n",
    "2. Travel through Json Data and for each tweet,\n",
    "> - Check if 'media' exists in entities. If yes, then I look for all media of type photos and increase my count for images.\n",
    "> - Append Relevant fields in the final_data list. If no images are present the write \"None\" as the last field.\n",
    "3. Convert to dataframe and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Text  \\\n",
      "0    RT @kdnuggets: Top 8 #Free Must-Read #Books on...   \n",
      "1    @nupur_baghel @PennDATS Congratulation @nupur_...   \n",
      "2    We have emailed the task details to all candid...   \n",
      "3    RT @rfpvjr: Our NAACL paper on polarization in...   \n",
      "4    RT @kdnuggets: Effective Transfer Learning For...   \n",
      "5    RT @stanfordnlp: Whatâ€™s new in @Stanford CS224...   \n",
      "6    RT @DeepMindAI: Today we're releasing a large-...   \n",
      "7    RT @ylecun: Congratulations Jitendra Malik !\\n...   \n",
      "8    RT @IIITDelhi: Another chance to take admissio...   \n",
      "9    Dear @midasIIITD internship candidates who hav...   \n",
      "10   Looking forward to your paper submission to @I...   \n",
      "11   RT @ngrams: Reproducibility in multimedia rese...   \n",
      "12   Online application for https://t.co/DJFDrQsHZP...   \n",
      "13   RT @ACMMM19: A final reminder of the Reproduci...   \n",
      "14   RT @isarth23: Thanks for the support and help ...   \n",
      "15   Since SemEval-2019 will be held June 6-7, 2019...   \n",
      "16   +@aggarwal_kartik.\\nCongrats! Wish you many mo...   \n",
      "17   RT @aggarwal_kartik: Our work (@midasIIITD ) a...   \n",
      "18   Congratulations! @midasIIITD team, @isarth23 @...   \n",
      "19   @EEMLcommunity @radamihalcea too many deadline...   \n",
      "20   RT @stanfordnlp: CS224N Natural Language Proce...   \n",
      "21   RT @ylecun: Learn PyTorch by running on Google...   \n",
      "22   Dr. Vineeth N Balasubramanian will present a T...   \n",
      "23   RT @ylecun: I am extremely honored to be the r...   \n",
      "24   Thanks to all shortlisted candidates for submi...   \n",
      "25   @IEEEBigMM19 @ACMMM19 and 6 days left for work...   \n",
      "26   RT @IEEEBigMM19: Hurry Up!\\n6 Days left for Ab...   \n",
      "27   Congratulations @midasIIITD students Simra Sha...   \n",
      "28   The last date for submitting a solution for th...   \n",
      "29   RT @IIITDelhi: @IIITDelhi invites application ...   \n",
      "..                                                 ...   \n",
      "299  RT @TensorFlow: TensorFlow 1.10.0 has been rel...   \n",
      "300  @midasIIITD is looking for motivated IIITD MTe...   \n",
      "301  @IIITDelhi @ponguru @RatnRajiv The results of ...   \n",
      "302  RT @IIITDelhi: @midasIIITD has secured rank 1 ...   \n",
      "303  RT @kdnuggets: Comparison of Top 6 Python NLP ...   \n",
      "304  Check more details of the 20th IEEE Internatio...   \n",
      "305  MR2AMC@ISM 2018 will be organized by @RatnRaji...   \n",
      "306  Our workshop proposal named, \"MR2AMC: Multimod...   \n",
      "307  @NUSComputing Congratulations Abdelhak and Pro...   \n",
      "308  RT @goodfellow_ian: One of the most anticipate...   \n",
      "309     @the_dhumketu Great to have you in @midasIIITD   \n",
      "310  Congratulation @soujanyaporia for being appoin...   \n",
      "311  @IIITDelhi @the_dhumketu Thanks team @midasIII...   \n",
      "312  RT @IIITDelhi: Congratulations @midasIIITD int...   \n",
      "313  RT @learning_pt: Profile of the 5 Indian under...   \n",
      "314  Have a look at the list of accepted papers in ...   \n",
      "315  RT @goodfellow_ian: https://t.co/hYiWI7ntyk Te...   \n",
      "316  RT @IIITDelhi: Congratulations Dr. @RatnRajiv ...   \n",
      "317  RT @ylecun: Jitendra Malik, who directs FAIR-M...   \n",
      "318  RT @kdnuggets: .@Bloomberg launches free cours...   \n",
      "319  RT @TechAtBloomberg: Missed #PyLondinium18? Wa...   \n",
      "320  RT @IIITDelhi: We are delighted to announce th...   \n",
      "321  Get ready for the annual technical fest of @II...   \n",
      "322  Congratulations Dr. @RatnRajiv and team @midas...   \n",
      "323  Congratulations MIDAS @midasIIITD intern Prakh...   \n",
      "324    MIDAS@IIITD foundation. https://t.co/LKuzyBHzjm   \n",
      "325  It feels great to be the part of @IIITDelhi. h...   \n",
      "326  Thank you, @toonzratn for designing the logo o...   \n",
      "327  We are on Facebook too. Like our page to get o...   \n",
      "328  MIDAS is a group of researchers at IIIT-Delhi ...   \n",
      "\n",
      "                               Date  Likes  Retweet Image Count  \n",
      "0    Sat Apr 06 17:11:29 +0000 2019      0        2        None  \n",
      "1    Sat Apr 06 16:43:27 +0000 2019     11        3        None  \n",
      "2    Fri Apr 05 16:08:37 +0000 2019      8        1        None  \n",
      "3    Fri Apr 05 04:05:11 +0000 2019      0       16        None  \n",
      "4    Fri Apr 05 04:04:43 +0000 2019      0       10           1  \n",
      "5    Wed Apr 03 18:31:53 +0000 2019      0       57        None  \n",
      "6    Wed Apr 03 17:04:32 +0000 2019      0      844        None  \n",
      "7    Wed Apr 03 09:03:40 +0000 2019      0       16        None  \n",
      "8    Wed Apr 03 07:46:02 +0000 2019      0        4        None  \n",
      "9    Tue Apr 02 04:20:13 +0000 2019      8        1        None  \n",
      "10   Tue Apr 02 02:44:54 +0000 2019      5        1        None  \n",
      "11   Tue Apr 02 02:35:44 +0000 2019      0        7        None  \n",
      "12   Mon Apr 01 06:53:08 +0000 2019      7        2        None  \n",
      "13   Sun Mar 31 10:21:24 +0000 2019      0       10        None  \n",
      "14   Fri Mar 29 19:43:24 +0000 2019      0        2        None  \n",
      "15   Fri Mar 29 17:16:40 +0000 2019      9        1        None  \n",
      "16   Fri Mar 29 17:04:30 +0000 2019      2        0        None  \n",
      "17   Fri Mar 29 17:03:29 +0000 2019      0        1        None  \n",
      "18   Fri Mar 29 17:02:24 +0000 2019      9        1        None  \n",
      "19   Fri Mar 29 05:35:22 +0000 2019      0        0        None  \n",
      "20   Thu Mar 28 16:55:01 +0000 2019      0      712        None  \n",
      "21   Thu Mar 28 16:54:37 +0000 2019      0      157        None  \n",
      "22   Wed Mar 27 16:09:09 +0000 2019      4        1        None  \n",
      "23   Wed Mar 27 11:53:40 +0000 2019      0     1544        None  \n",
      "24   Tue Mar 26 18:12:27 +0000 2019      5        1        None  \n",
      "25   Tue Mar 26 05:54:49 +0000 2019      2        0        None  \n",
      "26   Tue Mar 26 05:50:10 +0000 2019      0        3        None  \n",
      "27   Mon Mar 25 13:01:57 +0000 2019     18        1        None  \n",
      "28   Sun Mar 24 18:44:01 +0000 2019      8        3        None  \n",
      "29   Sun Mar 24 18:26:02 +0000 2019      0        4        None  \n",
      "..                              ...    ...      ...         ...  \n",
      "299  Thu Aug 09 05:59:57 +0000 2018      0      265        None  \n",
      "300  Wed Aug 08 11:30:56 +0000 2018      2        1        None  \n",
      "301  Wed Aug 08 05:53:48 +0000 2018      3        1           1  \n",
      "302  Wed Aug 08 05:45:58 +0000 2018      0        1        None  \n",
      "303  Tue Aug 07 07:16:33 +0000 2018      0       40           1  \n",
      "304  Tue Aug 07 02:05:12 +0000 2018      1        1        None  \n",
      "305  Tue Aug 07 01:58:49 +0000 2018      1        1        None  \n",
      "306  Tue Aug 07 01:50:33 +0000 2018      1        1        None  \n",
      "307  Mon Aug 06 17:48:23 +0000 2018      0        0        None  \n",
      "308  Mon Aug 06 17:46:59 +0000 2018      0      103           1  \n",
      "309  Mon Aug 06 06:06:47 +0000 2018      0        0        None  \n",
      "310  Fri Aug 03 05:56:33 +0000 2018      6        1        None  \n",
      "311  Wed Aug 01 11:47:15 +0000 2018      5        1           1  \n",
      "312  Wed Aug 01 11:20:07 +0000 2018      0        4        None  \n",
      "313  Wed Aug 01 05:06:47 +0000 2018      0        4        None  \n",
      "314  Tue Jul 31 12:11:52 +0000 2018      1        0        None  \n",
      "315  Tue Jul 31 02:06:26 +0000 2018      0      264        None  \n",
      "316  Mon Jul 30 07:30:51 +0000 2018      0        2        None  \n",
      "317  Sat Jul 28 11:07:11 +0000 2018      0       57        None  \n",
      "318  Sat Jul 28 06:14:09 +0000 2018      0      105        None  \n",
      "319  Sat Jul 28 06:13:48 +0000 2018      0        7        None  \n",
      "320  Sat Jul 28 04:08:21 +0000 2018      0        6        None  \n",
      "321  Fri Jul 27 06:46:44 +0000 2018      3        2        None  \n",
      "322  Fri Jul 27 04:07:31 +0000 2018      8        2        None  \n",
      "323  Wed Jul 25 05:14:35 +0000 2018      5        1        None  \n",
      "324  Tue Jul 24 10:33:23 +0000 2018      2        1        None  \n",
      "325  Tue Jul 24 10:12:34 +0000 2018      2        1        None  \n",
      "326  Tue Jul 24 09:46:26 +0000 2018      4        1           1  \n",
      "327  Mon Jul 23 16:25:05 +0000 2018      3        1        None  \n",
      "328  Mon Jul 23 12:53:15 +0000 2018      7        4        None  \n",
      "\n",
      "[329 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "final_data = []\n",
    "\n",
    "for i in jsondata:\n",
    "    ct=0\n",
    "    if('media' in i['entities']):\n",
    "        for j in i['entities']['media']:\n",
    "            if(j['type']=='photo'):\n",
    "                ct+=1\n",
    "    if ct==0:\n",
    "        final_data.append([i['text'],i['created_at'],i['favorite_count'],i['retweet_count'],\"None\"])\n",
    "    else:\n",
    "        final_data.append([i['text'],i['created_at'],i['favorite_count'],i['retweet_count'],ct])\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=final_data,columns=['Text','Date','Likes','Retweet','Image Count'])\n",
    "\n",
    "# print(jsondata[0])\n",
    "\n",
    "print (df)\n",
    "export_csv = df.to_csv (r'final.csv', index_label = [\"No.\"], header=True) #Don't forget to add '.csv' at the end of the path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
